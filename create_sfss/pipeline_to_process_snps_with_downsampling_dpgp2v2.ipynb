{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Pipeline 2: SNP processing with downsampling__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Requirements__\n",
    "- a python environment (installed with conda for example);\n",
    "- .TSV files from the obtained with pipeline 1;\n",
    "- python `subprocess`;\n",
    "- python `Numpy`;\n",
    "- python `Pandas`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from snp_utils import filter_short_introns_from_bed, filter_snps_by_interval, create_snp_total_counts_dict\n",
    "from sfs_utils import create_sfs_dict, downsample_sfs_in_dict\n",
    "from mutational_context_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Import the tables into Pandas__\n",
    "\n",
    "We are going to use pandas to import the SNP table. Pandas is a great (if used with cauting) Python package built on Numpy which allows easy dataFrame manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tur92196/WorkDir/DGN/dgrp2/masked/vcfs/tables\n",
      "NC_chr2L_ann_table.tsv NC_chr3L_ann_table.tsv\n",
      "NC_chr2R_ann_table.tsv NC_chr3R_ann_table.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/dmelnexus/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../../masked/vcfs/tables/\n",
    "!ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the `.tsv` file for each chromosome (except for the __chrom4__ and __chromX__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload files with pd.read_table()\n",
    "chr2L_table = pd.read_table(\"NC_chr2L_ann_table.tsv\")\n",
    "chr2R_table = pd.read_table(\"NC_chr2R_ann_table.tsv\")\n",
    "chr3L_table = pd.read_table(\"NC_chr3L_ann_table.tsv\")\n",
    "chr3R_table = pd.read_table(\"NC_chr3R_ann_table.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of SNPs in each file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chr2L = 901051, chr2R = 758465, chr3L = 919872, chr3R = 891691 SNPs!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the number of SNPs with .shape\n",
    "\"chr2L = {}, chr2R = {}, chr3L = {}, chr3R = {} SNPs!\".format(chr2L_table.shape[0], chr2R_table.shape[0], chr3L_table.shape[0], chr3R_table.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>pos</th>\n",
       "      <th>id</th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>refcount</th>\n",
       "      <th>altcount</th>\n",
       "      <th>refflank</th>\n",
       "      <th>altflank</th>\n",
       "      <th>refcodon</th>\n",
       "      <th>...</th>\n",
       "      <th>snpeff_trnscid</th>\n",
       "      <th>sift_trnscid</th>\n",
       "      <th>sift_geneid</th>\n",
       "      <th>sift_genename</th>\n",
       "      <th>sift_region</th>\n",
       "      <th>sift_vartype</th>\n",
       "      <th>sifts_core</th>\n",
       "      <th>sift_median</th>\n",
       "      <th>sift_pred</th>\n",
       "      <th>deleteriousness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr2L</td>\n",
       "      <td>130033</td>\n",
       "      <td>.</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>20</td>\n",
       "      <td>94</td>\n",
       "      <td>GTTGGCATG</td>\n",
       "      <td>GTTGACATG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>FBtr0078114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr2L</td>\n",
       "      <td>130039</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>ATGCTTTTA</td>\n",
       "      <td>ATGCATTTA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>FBtr0078114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr2L</td>\n",
       "      <td>130050</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>111</td>\n",
       "      <td>3</td>\n",
       "      <td>ATGCTCCGG</td>\n",
       "      <td>ATGCACCGG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>FBtr0078114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr2L</td>\n",
       "      <td>130196</td>\n",
       "      <td>.</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>CAAACCGCA</td>\n",
       "      <td>CAAAGCGCA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>FBtr0078114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr2L</td>\n",
       "      <td>130206</td>\n",
       "      <td>.</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>TTTGAGATA</td>\n",
       "      <td>TTTGGGATA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>FBtr0078114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   chrom     pos id ref alt  refcount  altcount   refflank   altflank  \\\n",
       "0  chr2L  130033  .   G   A        20        94  GTTGGCATG  GTTGACATG   \n",
       "1  chr2L  130039  .   T   A       112         1  ATGCTTTTA  ATGCATTTA   \n",
       "2  chr2L  130050  .   T   A       111         3  ATGCTCCGG  ATGCACCGG   \n",
       "3  chr2L  130196  .   C   G       113         1  CAAACCGCA  CAAAGCGCA   \n",
       "4  chr2L  130206  .   A   G       113         1  TTTGAGATA  TTTGGGATA   \n",
       "\n",
       "  refcodon  ... snpeff_trnscid sift_trnscid sift_geneid sift_genename  \\\n",
       "0      NaN  ...    FBtr0078114          NaN         NaN           NaN   \n",
       "1      NaN  ...    FBtr0078114          NaN         NaN           NaN   \n",
       "2      NaN  ...    FBtr0078114          NaN         NaN           NaN   \n",
       "3      NaN  ...    FBtr0078114          NaN         NaN           NaN   \n",
       "4      NaN  ...    FBtr0078114          NaN         NaN           NaN   \n",
       "\n",
       "  sift_region sift_vartype sifts_core sift_median sift_pred deleteriousness  \n",
       "0         NaN          NaN        NaN         NaN       NaN             NaN  \n",
       "1         NaN          NaN        NaN         NaN       NaN             NaN  \n",
       "2         NaN          NaN        NaN         NaN       NaN             NaN  \n",
       "3         NaN          NaN        NaN         NaN       NaN             NaN  \n",
       "4         NaN          NaN        NaN         NaN       NaN             NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at chr2L\n",
    "chr2L_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Prepare the DataFrame for downsampling__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total number of haplotypes for each SNP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a total count column to all datasets:\n",
    "chr2L_table['totalcount'] = chr2L_table['refcount'] + chr2L_table['altcount']\n",
    "chr2R_table['totalcount'] = chr2R_table['refcount'] + chr2R_table['altcount']\n",
    "chr3L_table['totalcount'] = chr3L_table['refcount'] + chr3L_table['altcount']\n",
    "chr3R_table['totalcount'] = chr3R_table['refcount'] + chr3R_table['altcount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>pos</th>\n",
       "      <th>id</th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>refcount</th>\n",
       "      <th>altcount</th>\n",
       "      <th>refflank</th>\n",
       "      <th>altflank</th>\n",
       "      <th>refcodon</th>\n",
       "      <th>...</th>\n",
       "      <th>sift_trnscid</th>\n",
       "      <th>sift_geneid</th>\n",
       "      <th>sift_genename</th>\n",
       "      <th>sift_region</th>\n",
       "      <th>sift_vartype</th>\n",
       "      <th>sifts_core</th>\n",
       "      <th>sift_median</th>\n",
       "      <th>sift_pred</th>\n",
       "      <th>deleteriousness</th>\n",
       "      <th>totalcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr2L</td>\n",
       "      <td>130033</td>\n",
       "      <td>.</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>20</td>\n",
       "      <td>94</td>\n",
       "      <td>GTTGGCATG</td>\n",
       "      <td>GTTGACATG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr2L</td>\n",
       "      <td>130039</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>ATGCTTTTA</td>\n",
       "      <td>ATGCATTTA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr2L</td>\n",
       "      <td>130050</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>111</td>\n",
       "      <td>3</td>\n",
       "      <td>ATGCTCCGG</td>\n",
       "      <td>ATGCACCGG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr2L</td>\n",
       "      <td>130196</td>\n",
       "      <td>.</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>CAAACCGCA</td>\n",
       "      <td>CAAAGCGCA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr2L</td>\n",
       "      <td>130206</td>\n",
       "      <td>.</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>TTTGAGATA</td>\n",
       "      <td>TTTGGGATA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   chrom     pos id ref alt  refcount  altcount   refflank   altflank  \\\n",
       "0  chr2L  130033  .   G   A        20        94  GTTGGCATG  GTTGACATG   \n",
       "1  chr2L  130039  .   T   A       112         1  ATGCTTTTA  ATGCATTTA   \n",
       "2  chr2L  130050  .   T   A       111         3  ATGCTCCGG  ATGCACCGG   \n",
       "3  chr2L  130196  .   C   G       113         1  CAAACCGCA  CAAAGCGCA   \n",
       "4  chr2L  130206  .   A   G       113         1  TTTGAGATA  TTTGGGATA   \n",
       "\n",
       "  refcodon  ... sift_trnscid sift_geneid sift_genename sift_region  \\\n",
       "0      NaN  ...          NaN         NaN           NaN         NaN   \n",
       "1      NaN  ...          NaN         NaN           NaN         NaN   \n",
       "2      NaN  ...          NaN         NaN           NaN         NaN   \n",
       "3      NaN  ...          NaN         NaN           NaN         NaN   \n",
       "4      NaN  ...          NaN         NaN           NaN         NaN   \n",
       "\n",
       "  sift_vartype sifts_core sift_median sift_pred deleteriousness totalcount  \n",
       "0          NaN        NaN         NaN       NaN             NaN        114  \n",
       "1          NaN        NaN         NaN       NaN             NaN        113  \n",
       "2          NaN        NaN         NaN       NaN             NaN        114  \n",
       "3          NaN        NaN         NaN       NaN             NaN        114  \n",
       "4          NaN        NaN         NaN       NaN             NaN        114  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at chr2L\n",
    "chr2L_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove SNPs if the totalcount < 160. This defines a lower bound for downsampling(the minimum number of haplotypes that we are downsampling higher number of haplotypes to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the lower bound for the number of haplotypes\n",
    "min_number_of_haplotypes = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr2L_table = chr2L_table[chr2L_table['totalcount'] >= min_number_of_haplotypes]\n",
    "chr2R_table = chr2R_table[chr2R_table['totalcount'] >= min_number_of_haplotypes]\n",
    "chr3L_table = chr3L_table[chr3L_table['totalcount'] >= min_number_of_haplotypes]\n",
    "chr3R_table = chr3R_table[chr3R_table['totalcount'] >= min_number_of_haplotypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chr2L = 567209, chr2R = 667855, chr3L = 711521, chr3R = 286307 SNPs!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the number of SNPs with .shape\n",
    "\"chr2L = {}, chr2R = {}, chr3L = {}, chr3R = {} SNPs!\".format(chr2L_table.shape[0], chr2R_table.shape[0], chr3L_table.shape[0], chr3R_table.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Subset SNPs__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Subset each chromosome to retain only:__\n",
    "- `SNPs in short-introns`;\n",
    "- `Synonymous SNPs`;\n",
    "- `Non-synonymous SNPs`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(1) Subset to retain only SNPs annotated as introns__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the other chromosomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to retain only SNPs annotated as introns\n",
    "chr2L_introns = chr2L_table[(chr2L_table['effect'] == \"INTRON\")]\n",
    "chr2R_introns = chr2R_table[(chr2R_table['effect'] == \"INTRON\")]\n",
    "chr3L_introns = chr3L_table[(chr3L_table['effect'] == \"INTRON\")]\n",
    "chr3R_introns = chr3R_table[(chr3R_table['effect'] == \"INTRON\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chr2L = 99046, chr2R = 116950, chr3L = 136576, chr3R = 58558 SNPs!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the number of SNPs with .shape\n",
    "\"chr2L = {}, chr2R = {}, chr3L = {}, chr3R = {} SNPs!\".format(chr2L_introns.shape[0], chr2R_introns.shape[0], chr3L_introns.shape[0], chr3R_introns.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(2)Keep only short intronic SNPs__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each chromosome, retain only SNPs in `short introns`. To do that, you need to identify the short-intros in Dm6 genome and head/tail the sequence to remove head and trailing 8bp from each short-intron. These extremes migh be under selection constraints. Download the intron regions as `.BED` file from [here](https://genome.ucsc.edu/cgi-bin/hgTables). Select *D. melanogaster* assembly known as *Dm6* (Aug. 2014, BDGP Release 6 + ISO 1 MT/dm6). Define the region of interest as `genome`, select the output format as `BED` and name it. In the next page, select only `Introns plus 0`, then hit `Get BED`. Now you are ready to go. The next step is to create a Pandas DataFrame with short introns intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tur92196/WorkDir/DGN/reference\n",
      "dm3.fa                            dmel-2L-chromosome-r5.57.fasta.gz\n",
      "dm3.fa.fai                        dmel-2L-r5.57.gff.gz\n",
      "dm3ToDm6.over.chain               dmel-2R-chromosome-r5.57.fasta.gz\n",
      "dm6.fa                            dmel-2R-r5.57.gff.gz\n",
      "dm6.fa.dict                       dmel-3L-chromosome-r5.57.fasta.gz\n",
      "dm6.fa.fai                        dmel-3L-r5.57.gff.gz\n",
      "dm6_introns.bed                   dmel-3R-chromosome-r5.57.fasta.gz\n",
      "dm6_short_introns.bed             dmel-3R-r5.57.gff.gz\n",
      "dmel-2L-chromosome-r5.57.fasta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/dmelnexus/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../../../../reference/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr2L</td>\n",
       "      <td>8387838</td>\n",
       "      <td>8387882</td>\n",
       "      <td>NM_001201797.2_intron_5_0_chr2L_8387832_f</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr2L</td>\n",
       "      <td>8387838</td>\n",
       "      <td>8387882</td>\n",
       "      <td>NM_001201795.2_intron_5_0_chr2L_8387832_f</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr2L</td>\n",
       "      <td>8387838</td>\n",
       "      <td>8387882</td>\n",
       "      <td>NM_164812.5_intron_4_0_chr2L_8387832_f</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr2L</td>\n",
       "      <td>8387838</td>\n",
       "      <td>8387882</td>\n",
       "      <td>NM_205936.3_intron_5_0_chr2L_8387832_f</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr2L</td>\n",
       "      <td>8387838</td>\n",
       "      <td>8387882</td>\n",
       "      <td>NM_205935.3_intron_5_0_chr2L_8387832_f</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1        2                                          3  4  5\n",
       "0  chr2L  8387838  8387882  NM_001201797.2_intron_5_0_chr2L_8387832_f  0  +\n",
       "1  chr2L  8387838  8387882  NM_001201795.2_intron_5_0_chr2L_8387832_f  0  +\n",
       "2  chr2L  8387838  8387882     NM_164812.5_intron_4_0_chr2L_8387832_f  0  +\n",
       "3  chr2L  8387838  8387882     NM_205936.3_intron_5_0_chr2L_8387832_f  0  +\n",
       "4  chr2L  8387838  8387882     NM_205935.3_intron_5_0_chr2L_8387832_f  0  +"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_introns = filter_short_introns_from_bed(\"dm6_introns.bed\", \n",
    "                                              \"dm6_short_introns.bed\", \n",
    "                                              [\"chr2L\", \"chr2R\", \"chr3L\", \"chr3R\"],\n",
    "                                              short_intron_size=86,\n",
    "                                              trailling_size=8)\n",
    "short_introns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, apply the filter based on the BED file intervals (to make sure to only keep `short-intronic` SNPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only short-introns SNPs.\n",
    "chr2L_short_introns = filter_snps_by_interval(chr2L_introns, short_introns)\n",
    "chr2R_short_introns = filter_snps_by_interval(chr2R_introns, short_introns)\n",
    "chr3L_short_introns = filter_snps_by_interval(chr3L_introns, short_introns)\n",
    "chr3R_short_introns = filter_snps_by_interval(chr3R_introns, short_introns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump these DataFrames as pickle files to avoid re-running it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tur92196/WorkDir/DGN/dgrp2\n",
      "dgrp2.txt \u001b[1m\u001b[36mdmel_data\u001b[m\u001b[m \u001b[1m\u001b[36mmasked\u001b[m\u001b[m    \u001b[1m\u001b[36moriginals\u001b[m\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/dmelnexus/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle chrm short-introns DataFrames\n",
    "# chr2L_short_introns.to_pickle(\"masked/vcfs/tables/NC_chr2L_short_introns_downsampled.pkl\")\n",
    "# chr2R_short_introns.to_pickle(\"masked/vcfs/tables/NC_chr2R_short_introns_downsampled.pkl\")\n",
    "# chr3L_short_introns.to_pickle(\"masked/vcfs/tables/NC_chr3L_short_introns_downsampled.pkl\")\n",
    "# chr3R_short_introns.to_pickle(\"masked/vcfs/tables/NC_chr3R_short_introns_downsampled.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled chrm short-introns DataFrames\n",
    "# chr2L_short_introns = pd.read_pickle(\"masked/vcfs/tables/NC_chr2L_short_introns_downsampled.pkl\")\n",
    "# chr2R_short_introns = pd.read_pickle(\"masked/vcfs/tables/NC_chr2R_short_introns_downsampled.pkl\")\n",
    "# chr3L_short_introns = pd.read_pickle(\"masked/vcfs/tables/NC_chr3L_short_introns_downsampled.pkl\")\n",
    "# chr3R_short_introns = pd.read_pickle(\"masked/vcfs/tables/NC_chr3R_short_introns_downsampled.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many short-introns SNPs were retained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chr2L = 1070, chr2R = 1278, chr3L = 967, chr3R = 631 SNPs!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the number of SNPs with .shape\n",
    "\"chr2L = {}, chr2R = {}, chr3L = {}, chr3R = {} SNPs!\".format(chr2L_short_introns.shape[0], chr2R_short_introns.shape[0], chr3L_short_introns.shape[0], chr3R_short_introns.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(3) Take synonymous and non-synonymous SNPs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset chr2L to retain only SNPs annotated as introns\n",
    "chr2L_exons = chr2L_table[(chr2L_table['effect'] == \"NON_SYNONYMOUS_CODING\") | (chr2L_table['effect'] == \"SYNONYMOUS_CODING\")]\n",
    "chr2R_exons = chr2R_table[(chr2R_table['effect'] == \"NON_SYNONYMOUS_CODING\") | (chr2R_table['effect'] == \"SYNONYMOUS_CODING\")]\n",
    "chr3L_exons = chr3L_table[(chr3L_table['effect'] == \"NON_SYNONYMOUS_CODING\") | (chr3L_table['effect'] == \"SYNONYMOUS_CODING\")]\n",
    "chr3R_exons = chr3R_table[(chr3R_table['effect'] == \"NON_SYNONYMOUS_CODING\") | (chr3R_table['effect'] == \"SYNONYMOUS_CODING\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle chrm exons DataFrames\n",
    "# chr2L_exons.to_pickle(\"masked/vcfs/tables/NC_chr2L_exons_downsampled.pkl\")\n",
    "# chr2R_exons.to_pickle(\"masked/vcfs/tables/NC_chr2R_exons_downsampled.pkl\")\n",
    "# chr3L_exons.to_pickle(\"masked/vcfs/tables/NC_chr3L_exons_downsampled.pkl\")\n",
    "# chr3R_exons.to_pickle(\"masked/vcfs/tables/NC_chr3R_exons_downsampled.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled chrm exons DataFrames\n",
    "# chr2L_exons = pd.read_pickle(\"masked/vcfs/tables/NC_chr2L_exons_downsampled.pkl\")\n",
    "# chr2R_exons = pd.read_pickle(\"masked/vcfs/tables/NC_chr2R_exons_downsampled.pkl\")\n",
    "# chr3L_exons = pd.read_pickle(\"masked/vcfs/tables/NC_chr3L_exons_downsampled.pkl\")\n",
    "# chr3R_exons = pd.read_pickle(\"masked/vcfs/tables/NC_chr3R_exons_downsampled.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chr2L = 93371, chr2R = 113816, chr3L = 103969, chr3R = 49499 SNPs!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the number of SNPs with .shape\n",
    "\"chr2L = {}, chr2R = {}, chr3L = {}, chr3R = {} SNPs!\".format(chr2L_exons.shape[0], chr2R_exons.shape[0], chr3L_exons.shape[0], chr3R_exons.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __SNPs counts dictionary__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of SNP counts for each dataFrame containing short-intron SNPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr2L_short_introns_dict = create_snp_total_counts_dict(chr2L_short_introns)\n",
    "chr2R_short_introns_dict = create_snp_total_counts_dict(chr2R_short_introns)\n",
    "chr3L_short_introns_dict = create_snp_total_counts_dict(chr3L_short_introns)\n",
    "chr3R_short_introns_dict = create_snp_total_counts_dict(chr3R_short_introns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then get the intron-SFS for each chromosome total count values (this is not downsampled yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chr2L_short_introns_raw_sfs = create_sfs_dict(chr2L_short_introns_dict)\n",
    "chr2R_short_introns_raw_sfs = create_sfs_dict(chr2R_short_introns_dict)\n",
    "chr3L_short_introns_raw_sfs = create_sfs_dict(chr3L_short_introns_dict)\n",
    "chr3R_short_introns_raw_sfs = create_sfs_dict(chr3R_short_introns_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr2L_short_introns_sfs = downsample_sfs_in_dict(chr2L_short_introns_raw_sfs, min_number_of_haplotypes, fold=True)\n",
    "chr2R_short_introns_sfs = downsample_sfs_in_dict(chr2R_short_introns_raw_sfs, min_number_of_haplotypes, fold=True)\n",
    "chr3L_short_introns_sfs = downsample_sfs_in_dict(chr3L_short_introns_raw_sfs, min_number_of_haplotypes, fold=True)\n",
    "chr3R_short_introns_sfs = downsample_sfs_in_dict(chr3R_short_introns_raw_sfs, min_number_of_haplotypes, fold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then combined each chromosome short-introns SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_introns_sfs_array = np.array([\n",
    "    chr2L_short_introns_sfs,\n",
    "    chr2R_short_introns_sfs,\n",
    "    chr3L_short_introns_sfs,\n",
    "    chr3R_short_introns_sfs\n",
    "])\n",
    "\n",
    "short_introns_sfs = np.sum(short_introns_sfs_array, 0).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the synonymous SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create total counts dictionary for synonymous SNPs\n",
    "chr2L_synonymous_dict = create_snp_total_counts_dict(chr2L_exons[(chr2L_exons['effect'] == \"SYNONYMOUS_CODING\")])\n",
    "chr2R_synonymous_dict = create_snp_total_counts_dict(chr2R_exons[(chr2R_exons['effect'] == \"SYNONYMOUS_CODING\")])\n",
    "chr3L_synonymous_dict = create_snp_total_counts_dict(chr3L_exons[(chr3L_exons['effect'] == \"SYNONYMOUS_CODING\")])\n",
    "chr3R_synonymous_dict = create_snp_total_counts_dict(chr3R_exons[(chr3R_exons['effect'] == \"SYNONYMOUS_CODING\")])\n",
    "\n",
    "# Create synonymous SFS dictionary\n",
    "chr2L_synonymous_raw_sfs = create_sfs_dict(chr2L_synonymous_dict)\n",
    "chr2R_synonymous_raw_sfs = create_sfs_dict(chr2R_synonymous_dict)\n",
    "chr3L_synonymous_raw_sfs = create_sfs_dict(chr3L_synonymous_dict)\n",
    "chr3R_synonymous_raw_sfs = create_sfs_dict(chr3R_synonymous_dict)\n",
    "\n",
    "# Then combined each chromosome synonymous SFS\n",
    "synonymous_sfs_array = np.array([\n",
    "    downsample_sfs_in_dict(chr2L_synonymous_raw_sfs, min_number_of_haplotypes, fold=True),\n",
    "    downsample_sfs_in_dict(chr2R_synonymous_raw_sfs, min_number_of_haplotypes, fold=True),\n",
    "    downsample_sfs_in_dict(chr3L_synonymous_raw_sfs, min_number_of_haplotypes, fold=True),\n",
    "    downsample_sfs_in_dict(chr3R_synonymous_raw_sfs, min_number_of_haplotypes, fold=True)\n",
    "])\n",
    "\n",
    "synonymous_sfs = np.sum(synonymous_sfs_array, 0).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the non-synonymous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create total counts dictionary for synonymous SNPs\n",
    "chr2L_nonsynonymous_dict = create_snp_total_counts_dict(chr2L_exons[(chr2L_exons['effect'] == \"NON_SYNONYMOUS_CODING\")])\n",
    "chr2R_nonsynonymous_dict = create_snp_total_counts_dict(chr2R_exons[(chr2R_exons['effect'] == \"NON_SYNONYMOUS_CODING\")])\n",
    "chr3L_nonsynonymous_dict = create_snp_total_counts_dict(chr3L_exons[(chr3L_exons['effect'] == \"NON_SYNONYMOUS_CODING\")])\n",
    "chr3R_nonsynonymous_dict = create_snp_total_counts_dict(chr3R_exons[(chr3R_exons['effect'] == \"NON_SYNONYMOUS_CODING\")])\n",
    "\n",
    "# Create synonymous SFS dictionary\n",
    "chr2L_nonsynonymous_raw_sfs = create_sfs_dict(chr2L_nonsynonymous_dict)\n",
    "chr2R_nonsynonymous_raw_sfs = create_sfs_dict(chr2R_nonsynonymous_dict)\n",
    "chr3L_nonsynonymous_raw_sfs = create_sfs_dict(chr3L_nonsynonymous_dict)\n",
    "chr3R_nonsynonymous_raw_sfs = create_sfs_dict(chr3R_nonsynonymous_dict)\n",
    "\n",
    "# Then combined each chromosome synonymous SFS\n",
    "nonsynonymous_sfs_array = np.array([\n",
    "    downsample_sfs_in_dict(chr2L_nonsynonymous_raw_sfs, min_number_of_haplotypes, fold=True),\n",
    "    downsample_sfs_in_dict(chr2R_nonsynonymous_raw_sfs, min_number_of_haplotypes, fold=True),\n",
    "    downsample_sfs_in_dict(chr3L_nonsynonymous_raw_sfs, min_number_of_haplotypes, fold=True),\n",
    "    downsample_sfs_in_dict(chr3R_nonsynonymous_raw_sfs, min_number_of_haplotypes, fold=True)\n",
    "])\n",
    "\n",
    "nonsynonymous_sfs = np.sum(nonsynonymous_sfs_array, 0).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the three SFSs to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tur92196/WorkDir/DGN/dgrp2/masked/vcfs\n",
      "NC_Chr2L.vcf NC_Chr3L.vcf NC_ChrX.vcf  \u001b[1m\u001b[36mfiltered\u001b[m\u001b[m     \u001b[1m\u001b[36mremade\u001b[m\u001b[m       \u001b[1m\u001b[36msnpeff\u001b[m\u001b[m\n",
      "NC_Chr2R.vcf NC_Chr3R.vcf chrms        \u001b[1m\u001b[36mliftover\u001b[m\u001b[m     \u001b[1m\u001b[36msift4g\u001b[m\u001b[m       \u001b[1m\u001b[36mtables\u001b[m\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/dmelnexus/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd masked/vcfs/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Save the annotated SNP files into snpeff folder\n",
    "sfss_folder=\"sfss\"\n",
    "if [ !  -d \"$sfss_folder\" ]; \n",
    "then\n",
    "    mkdir -p $sfss_folder && ls $sfss_folder\n",
    "else\n",
    "    ls $sfss_folder\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tur92196/WorkDir/DGN/dgrp2/masked/vcfs/sfss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/dmelnexus/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd sfss/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sfs_file = \"no-pairing/NC_sfs_si_nopairing_downsampled_folded.txt\"\n",
    "\n",
    "with open(output_sfs_file, \"w\") as of:\n",
    "    of.write(\"Introns, synonymous, and nonsynonymous SFS of \" + str(min_number_of_haplotypes) + \" samples\" + \"\\n\")\n",
    "    of.write(\"\\t\".join(str(item) for item in short_introns_sfs) + \"\\n\")\n",
    "    of.write(\"\\n\")\n",
    "    of.write(\"\\t\".join(str(item) for item in synonymous_sfs) + \"\\n\")\n",
    "    of.write(\"\\n\")\n",
    "    of.write(\"\\t\".join(str(item) for item in nonsynonymous_sfs) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Pair neutral and non-neutral SNPs by mutational context__\n",
    "\n",
    "Now that we had a simple way to get the SFSs, we are going to add some complexities and pair neutral and non-neutral SNPs by their mutational context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mno-pairing\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove SNPs that are close to each other, as they might cause ambiguity on the sequence category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Short-introns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get True for SNPs far apart and False otherwise, and insert a column on each chromosome DataFrame:\n",
    "# Insert the new column at position 3\n",
    "chr2L_short_introns.insert(2, \"pos_to_keep\", find_consecutive_positions(list(chr2L_short_introns[\"pos\"])))\n",
    "chr2R_short_introns.insert(2, \"pos_to_keep\", find_consecutive_positions(list(chr2R_short_introns[\"pos\"])))\n",
    "chr3L_short_introns.insert(2, \"pos_to_keep\", find_consecutive_positions(list(chr3L_short_introns[\"pos\"])))\n",
    "chr3R_short_introns.insert(2, \"pos_to_keep\", find_consecutive_positions(list(chr3R_short_introns[\"pos\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr2L_short_introns = chr2L_short_introns[chr2L_short_introns['pos_to_keep'] == True]\n",
    "chr2R_short_introns = chr2R_short_introns[chr2R_short_introns['pos_to_keep'] == True]\n",
    "chr3L_short_introns = chr3L_short_introns[chr3L_short_introns['pos_to_keep'] == True]\n",
    "chr3R_short_introns = chr3R_short_introns[chr3R_short_introns['pos_to_keep'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chr2L = 864, chr2R = 1049, chr3L = 801, chr3R = 528 SNPs!'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the number of SNPs with .shape\n",
    "\"chr2L = {}, chr2R = {}, chr3L = {}, chr3R = {} SNPs!\".format(chr2L_short_introns.shape[0], chr2R_short_introns.shape[0], chr3L_short_introns.shape[0], chr3R_short_introns.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Exons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get True for SNPs far apart and False otherwise, and insert a column on each chromosome DataFrame:\n",
    "# Insert the new column at position 3\n",
    "chr2L_exons.insert(2, \"pos_to_keep\", find_consecutive_positions(list(chr2L_exons[\"pos\"])))\n",
    "chr2R_exons.insert(2, \"pos_to_keep\", find_consecutive_positions(list(chr2R_exons[\"pos\"])))\n",
    "chr3L_exons.insert(2, \"pos_to_keep\", find_consecutive_positions(list(chr3L_exons[\"pos\"])))\n",
    "chr3R_exons.insert(2, \"pos_to_keep\", find_consecutive_positions(list(chr3R_exons[\"pos\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr2L_exons = chr2L_exons[chr2L_exons['pos_to_keep'] == True]\n",
    "chr2R_exons = chr2R_exons[chr2R_exons['pos_to_keep'] == True]\n",
    "chr3L_exons = chr3L_exons[chr3L_exons['pos_to_keep'] == True]\n",
    "chr3R_exons = chr3R_exons[chr3R_exons['pos_to_keep'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chr2L = 86992, chr2R = 106007, chr3L = 96782, chr3R = 46619 SNPs!'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the number of SNPs with .shape\n",
    "\"chr2L = {}, chr2R = {}, chr3L = {}, chr3R = {} SNPs!\".format(chr2L_exons.shape[0], chr2R_exons.shape[0], chr3L_exons.shape[0], chr3R_exons.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add the sequence category for each SNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the new column at position 9\n",
    "# For introns:\n",
    "chr2L_short_introns.insert(8, \"sequence_category\", set_snp_sequence_category(chr2L_short_introns))\n",
    "chr2R_short_introns.insert(8, \"sequence_category\", set_snp_sequence_category(chr2R_short_introns))\n",
    "chr3L_short_introns.insert(8, \"sequence_category\", set_snp_sequence_category(chr3L_short_introns))\n",
    "chr3R_short_introns.insert(8, \"sequence_category\", set_snp_sequence_category(chr3R_short_introns))\n",
    "\n",
    "# For exons:\n",
    "chr2L_exons.insert(8, \"sequence_category\", set_snp_sequence_category(chr2L_exons))\n",
    "chr2R_exons.insert(8, \"sequence_category\", set_snp_sequence_category(chr2R_exons))\n",
    "chr3L_exons.insert(8, \"sequence_category\", set_snp_sequence_category(chr3L_exons))\n",
    "chr3R_exons.insert(8, \"sequence_category\", set_snp_sequence_category(chr3R_exons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the sequence category dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ['AA/CA', 'AC/AA'], 2: ['CA/CA', 'CC/AA'], 3: ['GA/CA', 'GC/AA'], 4: ['TA/CA', 'TC/AA'], 5: ['AA/CC', 'AC/AC'], 6: ['CA/CC', 'CC/AC'], 7: ['GA/CC', 'GC/AC'], 8: ['TA/CC', 'TC/AC'], 9: ['AA/CG', 'AC/AG'], 10: ['CA/CG', 'CC/AG'], 11: ['GA/CG', 'GC/AG'], 12: ['TA/CG', 'TC/AG'], 13: ['AA/CT', 'AC/AT'], 14: ['CA/CT', 'CC/AT'], 15: ['GA/CT', 'GC/AT'], 16: ['TA/CT', 'TC/AT'], 17: ['AA/GA', 'AG/AA'], 18: ['CA/GA', 'CG/AA'], 19: ['GA/GA', 'GG/AA'], 20: ['TA/GA', 'TG/AA'], 21: ['AA/GC', 'AG/AC'], 22: ['CA/GC', 'CG/AC'], 23: ['GA/GC', 'GG/AC'], 24: ['TA/GC', 'TG/AC'], 25: ['AA/GG', 'AG/AG'], 26: ['CA/GG', 'CG/AG'], 27: ['GA/GG', 'GG/AG'], 28: ['TA/GG', 'TG/AG'], 29: ['AA/GT', 'AG/AT'], 30: ['CA/GT', 'CG/AT'], 31: ['GA/GT', 'GG/AT'], 32: ['TA/GT', 'TG/AT'], 33: ['AA/TA', 'AT/AA'], 34: ['CA/TA', 'CT/AA'], 35: ['GA/TA', 'GT/AA'], 36: ['TA/TA', 'TT/AA'], 37: ['AA/TC', 'AT/AC'], 38: ['CA/TC', 'CT/AC'], 39: ['GA/TC', 'GT/AC'], 40: ['TA/TC', 'TT/AC'], 41: ['AA/TG', 'AT/AG'], 42: ['CA/TG', 'CT/AG'], 43: ['GA/TG', 'GT/AG'], 44: ['TA/TG', 'TT/AG'], 45: ['AA/TT', 'AT/AT'], 46: ['CA/TT', 'CT/AT'], 47: ['GA/TT', 'GT/AT'], 48: ['TA/TT', 'TT/AT'], 49: ['AC/GA', 'AG/CA'], 50: ['CC/GA', 'CG/CA'], 51: ['GC/GA', 'GG/CA'], 52: ['TC/GA', 'TG/CA'], 53: ['AC/GC', 'AG/CC'], 54: ['CC/GC', 'CG/CC'], 55: ['GC/GC', 'GG/CC'], 56: ['TC/GC', 'TG/CC'], 57: ['AC/GG', 'AG/CG'], 58: ['CC/GG', 'CG/CG'], 59: ['GC/GG', 'GG/CG'], 60: ['TC/GG', 'TG/CG'], 61: ['AC/GT', 'AG/CT'], 62: ['CC/GT', 'CG/CT'], 63: ['GC/GT', 'GG/CT'], 64: ['TC/GT', 'TG/CT'], 65: ['AC/TA', 'AT/CA'], 66: ['CC/TA', 'CT/CA'], 67: ['GC/TA', 'GT/CA'], 68: ['TC/TA', 'TT/CA'], 69: ['AC/TC', 'AT/CC'], 70: ['CC/TC', 'CT/CC'], 71: ['GC/TC', 'GT/CC'], 72: ['TC/TC', 'TT/CC'], 73: ['AC/TG', 'AT/CG'], 74: ['CC/TG', 'CT/CG'], 75: ['GC/TG', 'GT/CG'], 76: ['TC/TG', 'TT/CG'], 77: ['AC/TT', 'AT/CT'], 78: ['CC/TT', 'CT/CT'], 79: ['GC/TT', 'GT/CT'], 80: ['TC/TT', 'TT/CT'], 81: ['AG/TA', 'AT/GA'], 82: ['CG/TA', 'CT/GA'], 83: ['GG/TA', 'GT/GA'], 84: ['TG/TA', 'TT/GA'], 85: ['AG/TC', 'AT/GC'], 86: ['CG/TC', 'CT/GC'], 87: ['GG/TC', 'GT/GC'], 88: ['TG/TC', 'TT/GC'], 89: ['AG/TG', 'AT/GG'], 90: ['CG/TG', 'CT/GG'], 91: ['GG/TG', 'GT/GG'], 92: ['TG/TG', 'TT/GG'], 93: ['AG/TT', 'AT/GT'], 94: ['CG/TT', 'CT/GT'], 95: ['GG/TT', 'GT/GT'], 96: ['TG/TT', 'TT/GT']}\n"
     ]
    }
   ],
   "source": [
    "# First create a object for the sequence categories dictionary\n",
    "sequence_categories_dict = create_sequence_categories_dict()\n",
    "print(sequence_categories_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create three dictionaries for each chromosome\n",
    "# For introns\n",
    "chr2L_short_introns_seqclasses = create_snp_dict_wrapper(chr2L_short_introns, sequence_categories_dict, \"introns\")\n",
    "chr2R_short_introns_seqclasses = create_snp_dict_wrapper(chr2R_short_introns, sequence_categories_dict, \"introns\")\n",
    "chr3L_short_introns_seqclasses = create_snp_dict_wrapper(chr3L_short_introns, sequence_categories_dict, \"introns\")\n",
    "chr3R_short_introns_seqclasses = create_snp_dict_wrapper(chr3R_short_introns, sequence_categories_dict, \"introns\")\n",
    "\n",
    "# For exons\n",
    "chr2L_nonsyns_seqclasses, chr2L_syns_seqclasses = create_snp_dict_wrapper(chr2L_exons, sequence_categories_dict, \"exons\")\n",
    "chr2R_nonsyns_seqclasses, chr2R_syns_seqclasses = create_snp_dict_wrapper(chr2R_exons, sequence_categories_dict, \"exons\")\n",
    "chr3L_nonsyns_seqclasses, chr3L_syns_seqclasses = create_snp_dict_wrapper(chr3L_exons, sequence_categories_dict, \"exons\")\n",
    "chr3R_nonsyns_seqclasses, chr3R_syns_seqclasses = create_snp_dict_wrapper(chr3R_exons, sequence_categories_dict, \"exons\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find mutational context pairs and get the SFSs\n",
    "Now, find the pairs and obtain the SFSs for each member of the pair. These are the valid pairs we are looking to have:\n",
    "- `short-intron SNP` and `non-synonymous SNPs`;\n",
    "- `short-intron SNP` and `synonymous SNPs`;\n",
    "- `synonymous SNP` and `non-synonymous SNPs` (maybe?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `short-intron SNP` and `non-synonymous SNPs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find pairs of SNPs that are closest to each other\n",
    "chr2L_si_nonsyn_pairs_si, chr2L_si_nonsyn_pairs_nonsyn = find_closest_snp_pairs(chr2L_short_introns_seqclasses, chr2L_nonsyns_seqclasses)\n",
    "chr2R_si_nonsyn_pairs_si, chr2R_si_nonsyn_pairs_nonsyn = find_closest_snp_pairs(chr2R_short_introns_seqclasses, chr2R_nonsyns_seqclasses)\n",
    "chr3L_si_nonsyn_pairs_si, chr3L_si_nonsyn_pairs_nonsyn = find_closest_snp_pairs(chr3L_short_introns_seqclasses, chr3L_nonsyns_seqclasses)\n",
    "chr3R_si_nonsyn_pairs_si, chr3R_si_nonsyn_pairs_nonsyn = find_closest_snp_pairs(chr3R_short_introns_seqclasses, chr3R_nonsyns_seqclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n"
     ]
    }
   ],
   "source": [
    "# Get the SFS for the short introns SNPs\n",
    "# Remember: the data is now sampled, so the max_sample_size means the lower bound set to 160.\n",
    "# Use the imputed argument, knowing that the data was sampled.\n",
    "short_introns_paired_nonsyn_sfs_seqclasses_array = np.array([\n",
    "    create_unfolded_sfs_from_snp_dict(chr2L_si_nonsyn_pairs_si, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr2R_si_nonsyn_pairs_si, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr3L_si_nonsyn_pairs_si, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr3R_si_nonsyn_pairs_si, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True)\n",
    "])\n",
    "\n",
    "short_introns_paired_nonsyn_seqclasses_sfs_folded = np.sum(short_introns_paired_nonsyn_sfs_seqclasses_array, 0).tolist()\n",
    "\n",
    "# Get the SFS for the nonsynonymous SNPs\n",
    "nonsynonymous_paired_si_sfs_seqclasses_array = np.array([\n",
    "    create_unfolded_sfs_from_snp_dict(chr2L_si_nonsyn_pairs_nonsyn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr2R_si_nonsyn_pairs_nonsyn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr3L_si_nonsyn_pairs_nonsyn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr3R_si_nonsyn_pairs_nonsyn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True)\n",
    "])\n",
    "\n",
    "nonsynonymous_paired_si_seqclasses_sfs_folded = np.sum(nonsynonymous_paired_si_sfs_seqclasses_array, 0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pairs to a file\n",
    "si_nonsyn_pair_output_sfs_file = \"paired/NC_sfs_si_paired_with_nonsynonymous_downsampled_folded.txt\"\n",
    "\n",
    "with open(si_nonsyn_pair_output_sfs_file, \"w\") as of:\n",
    "    of.write(\"Introns and nonsynonymous SFS of \" + str(min_number_of_haplotypes) + \" samples\" + \"\\n\")\n",
    "    of.write(\"\\t\".join(str(item) for item in short_introns_paired_nonsyn_seqclasses_sfs_folded) + \"\\n\")\n",
    "    of.write(\"\\n\")\n",
    "    of.write(\"\\t\".join(str(item) for item in nonsynonymous_paired_si_seqclasses_sfs_folded) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `short-intron SNP` and `synonymous SNPs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find pairs of SNPs that are closest to each other\n",
    "chr2L_si_syn_pairs_si, chr2L_si_syn_pairs_syn = find_closest_snp_pairs(chr2L_short_introns_seqclasses, chr2L_syns_seqclasses)\n",
    "chr2R_si_syn_pairs_si, chr2R_si_syn_pairs_syn = find_closest_snp_pairs(chr2R_short_introns_seqclasses, chr2R_syns_seqclasses)\n",
    "chr3L_si_syn_pairs_si, chr3L_si_syn_pairs_syn = find_closest_snp_pairs(chr3L_short_introns_seqclasses, chr3L_syns_seqclasses)\n",
    "chr3R_si_syn_pairs_si, chr3R_si_syn_pairs_syn = find_closest_snp_pairs(chr3R_short_introns_seqclasses, chr3R_syns_seqclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n"
     ]
    }
   ],
   "source": [
    "# Get the SFS for the short introns SNPs\n",
    "short_introns_paired_syn_sfs_seqclasses_array = np.array([\n",
    "    create_unfolded_sfs_from_snp_dict(chr2L_si_syn_pairs_si, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr2R_si_syn_pairs_si, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr3L_si_syn_pairs_si, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr3R_si_syn_pairs_si, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True)\n",
    "])\n",
    "\n",
    "short_introns_paired_syn_seqclasses_sfs_folded = np.sum(short_introns_paired_syn_sfs_seqclasses_array, 0).tolist()\n",
    "\n",
    "# Get the SFS for the synonymous SNPs\n",
    "synonymous_paired_si_sfs_seqclasses_array = np.array([\n",
    "    create_unfolded_sfs_from_snp_dict(chr2L_si_syn_pairs_syn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr2R_si_syn_pairs_syn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr3L_si_syn_pairs_syn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr3R_si_syn_pairs_syn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True)\n",
    "])\n",
    "\n",
    "synonymous_paired_si_seqclasses_sfs_folded = np.sum(synonymous_paired_si_sfs_seqclasses_array, 0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pairs to a file\n",
    "si_syn_pair_output_sfs_file = \"paired/NC_sfs_si_paired_with_synonymous_downsampled_folded_.txt\"\n",
    "\n",
    "with open(si_syn_pair_output_sfs_file, \"w\") as of:\n",
    "    of.write(\"Introns and synonymous SFS of \" + str(min_number_of_haplotypes) + \" samples\" + \"\\n\")\n",
    "    of.write(\"\\t\".join(str(item) for item in short_introns_paired_syn_seqclasses_sfs_folded) + \"\\n\")\n",
    "    of.write(\"\\n\")\n",
    "    of.write(\"\\t\".join(str(item) for item in synonymous_paired_si_seqclasses_sfs_folded) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `synonymous SNP` and `non-synonymous SNPs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find pairs of SNPs that are closest to each other\n",
    "chr2L_syn_nonsyn_pairs_syn, chr2L_syn_nonsyn_pairs_nonsyn = find_closest_snp_pairs(chr2L_syns_seqclasses, chr2L_nonsyns_seqclasses)\n",
    "chr2R_syn_nonsyn_pairs_syn, chr2R_syn_nonsyn_pairs_nonsyn = find_closest_snp_pairs(chr2R_syns_seqclasses, chr2R_nonsyns_seqclasses)\n",
    "chr3L_syn_nonsyn_pairs_syn, chr3L_syn_nonsyn_pairs_nonsyn = find_closest_snp_pairs(chr3L_syns_seqclasses, chr3L_nonsyns_seqclasses)\n",
    "chr3R_syn_nonsyn_pairs_syn, chr3R_syn_nonsyn_pairs_nonsyn = find_closest_snp_pairs(chr3R_syns_seqclasses, chr3R_nonsyns_seqclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n",
      "Working with downsampled data...\n"
     ]
    }
   ],
   "source": [
    "# Get the SFS for the synonymous SNPs\n",
    "synonymous_paired_nonsyn_sfs_seqclasses_array = np.array([\n",
    "    create_unfolded_sfs_from_snp_dict(chr2L_syn_nonsyn_pairs_syn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr2R_syn_nonsyn_pairs_syn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr3L_syn_nonsyn_pairs_syn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr3R_syn_nonsyn_pairs_syn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True)\n",
    "])\n",
    "\n",
    "synonymous_paired_nonsyn_seqclasses_sfs_folded = np.sum(synonymous_paired_nonsyn_sfs_seqclasses_array, 0).tolist()\n",
    "\n",
    "# Get the SFS for the nonsynonymous SNPs\n",
    "nonsynonymous_paired_syn_sfs_seqclasses_array = np.array([\n",
    "    create_unfolded_sfs_from_snp_dict(chr2L_syn_nonsyn_pairs_nonsyn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr2R_syn_nonsyn_pairs_nonsyn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr3L_syn_nonsyn_pairs_nonsyn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True),\n",
    "    create_unfolded_sfs_from_snp_dict(chr3R_syn_nonsyn_pairs_nonsyn, \"downsampled\", max_sample_size=189, min_sample_size=min_number_of_haplotypes, folded=True)\n",
    "])\n",
    "\n",
    "nonsynonymous_paired_syn_seqclasses_sfs_folded = np.sum(nonsynonymous_paired_syn_sfs_seqclasses_array, 0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pairs to a file\n",
    "syn_nonsyn_pair_output_sfs_file = \"paired/NC_sfs_synonymous_paired_with_nonsynnonymous_downsampled_folded.txt\"\n",
    "\n",
    "with open(syn_nonsyn_pair_output_sfs_file, \"w\") as of:\n",
    "    of.write(\"Synonymous and nonsynonymous SFS of \" + str(min_number_of_haplotypes) + \" samples\" + \"\\n\")\n",
    "    of.write(\"\\t\".join(str(item) for item in synonymous_paired_nonsyn_seqclasses_sfs_folded) + \"\\n\")\n",
    "    of.write(\"\\n\")\n",
    "    of.write(\"\\t\".join(str(item) for item in nonsynonymous_paired_syn_seqclasses_sfs_folded) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
